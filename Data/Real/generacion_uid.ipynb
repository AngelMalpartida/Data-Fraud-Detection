{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0371967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a17bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado con éxito. Primeras filas:\n",
      "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
      "0        2987000        0          86400            68.5         W  13926   \n",
      "1        2987001        0          86401            29.0         W   2755   \n",
      "2        2987002        0          86469            59.0         W   4663   \n",
      "3        2987003        0          86499            50.0         W  18132   \n",
      "4        2987004        0          86506            50.0         H   4497   \n",
      "\n",
      "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
      "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
      "\n",
      "  V336  V337  V338  V339  \n",
      "0  NaN   NaN   NaN   NaN  \n",
      "1  NaN   NaN   NaN   NaN  \n",
      "2  NaN   NaN   NaN   NaN  \n",
      "3  NaN   NaN   NaN   NaN  \n",
      "4  0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 394 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset original\n",
    "df = pd.read_csv('credit_dataset_2_transaccion.csv')\n",
    "print(\"Dataset cargado con éxito. Primeras filas:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb19e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'uid' generada con éxito. Primeras filas:\n",
      "   TransactionID      uid\n",
      "0        2987000  2987000\n",
      "1        2987001  2987001\n",
      "2        2987002  2987002\n",
      "3        2987003  2987003\n",
      "4        2987004  2987004\n"
     ]
    }
   ],
   "source": [
    "# Crear columnas necesarias para el cálculo de UID\n",
    "df['uid_td_D1'] = np.floor(df['TransactionDT'] / (24 * 60 * 60))  # Calcular días desde TransactionDT\n",
    "\n",
    "# Inicializar UID con TransactionID\n",
    "df['uid'] = np.nan\n",
    "\n",
    "# Lógica para calcular UID agrupando por 'card1' y 'uid_td_D1'\n",
    "df['uid'] = df.groupby(['card1', 'uid_td_D1'])['TransactionID'].transform('min')\n",
    "\n",
    "# Filtrar las columnas necesarias para exportar\n",
    "df['uid'] = df.groupby(['card1', 'uid_td_D1'])['TransactionID'].transform('min')\n",
    "\n",
    "print(\"Columna 'uid' generada con éxito. Primeras filas:\")\n",
    "print(df[['TransactionID', 'uid']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aa6c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo exportado: credit_dataset_2_transaccion_with_uid.csv\n"
     ]
    }
   ],
   "source": [
    "# Exportar el DataFrame con la nueva columna UID\n",
    "df.to_csv('credit_dataset_2_transaccion_with_uid.csv', index=False)\n",
    "print(\"Archivo exportado: credit_dataset_2_transaccion_with_uid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51be5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION MAS COMPLETA\n",
    "# Código adaptado para generar UID a partir de:\n",
    "# credit_dataset_2_transaccion.csv (debe estar en la misma carpeta)\n",
    "# Basado en el script original (script generar uid.txt). :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------\n",
    "# 1) Lectura / carga\n",
    "# -------------------------\n",
    "def load_csv(path='credit_dataset_2_transaccion.csv', nrows=None, local_test=False):\n",
    "    \"\"\"\n",
    "    Carga el CSV. Si local_test=True limita rows para pruebas rápidas.\n",
    "    \"\"\"\n",
    "    print(\"Leyendo:\", path)\n",
    "    df = pd.read_csv(path, nrows=nrows)\n",
    "    if local_test:\n",
    "        df = df.head(10000)\n",
    "        print(\"Modo local_test: tomando primeras\", len(df), \"filas\")\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# 2) Preprocesamiento base (crea campos uid_td_*, DT_day, fixes)\n",
    "# -------------------------\n",
    "def preprocess_for_uid(df):\n",
    "    \"\"\"\n",
    "    Crea columnas necesarias para la lógica de uid tal como en el notebook original:\n",
    "      - uid_td_D1, uid_td_D2, uid_td_D3, uid_td_D5, uid_td_D10, uid_td_D11, uid_td_D15\n",
    "      - DT_day\n",
    "      - TransactionAmt_fix, V313_fix\n",
    "      - lista v_cols (las Vx que tienen decimales y se usan como 'fix' en el original)\n",
    "    Devuelve df modificado + lista v_cols detectadas.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # full_addr\n",
    "    if 'addr1' in df.columns and 'addr2' in df.columns:\n",
    "        df['full_addr'] = df['addr1'].astype(str) + '_' + df['addr2'].astype(str)\n",
    "    else:\n",
    "        df['full_addr'] = np.nan\n",
    "\n",
    "    # columnas D* usadas en el script original\n",
    "    D_list = ['D1','D2','D3','D5','D10','D11','D15']\n",
    "    for col in D_list:\n",
    "        if col in df.columns:\n",
    "            new_col = 'uid_td_' + col\n",
    "            # cuidado con NaNs: si hay NaN en D*, lo dejamos NaN\n",
    "            df[new_col] = np.nan\n",
    "            mask_ok = df[col].notna() & df['TransactionDT'].notna()\n",
    "            if mask_ok.any():\n",
    "                # cálculo: floor(TransactionDT / (24*60*60) - D) + 1000\n",
    "                df.loc[mask_ok, new_col] = np.floor(df.loc[mask_ok, 'TransactionDT']/(24*60*60) - df.loc[mask_ok, col]) + 1000\n",
    "        else:\n",
    "            df['uid_td_' + col] = np.nan\n",
    "\n",
    "    # DT_day\n",
    "    if 'TransactionDT' in df.columns:\n",
    "        df['DT_day'] = np.floor(df['TransactionDT']/(24*60*60)) + 1000\n",
    "    else:\n",
    "        df['DT_day'] = np.nan\n",
    "\n",
    "    # fixes\n",
    "    if 'TransactionAmt' in df.columns:\n",
    "        df['TransactionAmt_fix'] = df['TransactionAmt'].round(2)\n",
    "    else:\n",
    "        df['TransactionAmt_fix'] = 0.0\n",
    "\n",
    "    if 'V313' in df.columns:\n",
    "        df['V313_fix'] = df['V313'].round(2)\n",
    "    else:\n",
    "        df['V313_fix'] = 0.0\n",
    "\n",
    "    # placeholder uid\n",
    "    df['uid'] = np.nan\n",
    "\n",
    "    # detectamos v_cols -> comportamiento parecido al script original:\n",
    "    v_cols = []\n",
    "    v_fix_cols = []\n",
    "    for i in range(1,340):\n",
    "        col = f'V{i}'\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        # si tiene décimales (no son enteros) lo marcamos como v_cols (omitiendo V313)\n",
    "        col_vals = df[col].fillna(0)\n",
    "        # si la resta de float-int no es 0 => hay fracción\n",
    "        if ((col_vals - col_vals.astype(int)).abs().sum() != 0) and (col != 'V313'):\n",
    "            v_cols.append(col)\n",
    "            v_fix_cols.append(col + '_fix')\n",
    "            # reproducimos la idea del script original: col_fix_ground y col_fix\n",
    "            df[col + '_fix_ground'] = df[col].round(2)\n",
    "            df[col + '_fix'] = df[col + '_fix_ground'] + df['TransactionAmt_fix']\n",
    "\n",
    "    return df, v_cols\n",
    "\n",
    "# -------------------------\n",
    "# 3) Algoritmos principales para asignar uid (versión simplificada)\n",
    "# -------------------------\n",
    "def assign_single_items(df):\n",
    "    \"\"\"\n",
    "    Marca items 'single' (solo una transacción por card1 + uid_td_D1),\n",
    "    y les asigna uid = TransactionID (como en el script).\n",
    "    Devuelve single_items (con uid) y df sin esos items (all_items).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if not {'card1', 'uid_td_D1', 'TransactionID'}.issubset(df.columns):\n",
    "        raise ValueError(\"Faltan columnas necesarias (card1, uid_td_D1, TransactionID)\")\n",
    "\n",
    "    df['count_tmp'] = df.groupby(['card1', 'uid_td_D1'])['TransactionID'].transform('count')\n",
    "    single_items = df[df['count_tmp'] == 1].copy()\n",
    "    if len(single_items) > 0:\n",
    "        single_items['uid'] = single_items['TransactionID']\n",
    "\n",
    "    all_items = df[~df['TransactionID'].isin(single_items['TransactionID'])].copy()\n",
    "    # clean tmp col\n",
    "    single_items.drop(columns=['count_tmp'], inplace=True, errors='ignore')\n",
    "    all_items.drop(columns=['count_tmp'], inplace=True, errors='ignore')\n",
    "    return single_items, all_items\n",
    "\n",
    "def assign_first_appearances(all_items):\n",
    "    \"\"\"\n",
    "    Para cada grupo (card1, uid_td_D1) toma la primera aparición (cumcount==0)\n",
    "    y le asigna uid = TransactionID (como 'first_df' en el script).\n",
    "    \"\"\"\n",
    "    df = all_items.copy()\n",
    "    if not {'card1', 'uid_td_D1', 'TransactionID'}.issubset(df.columns):\n",
    "        raise ValueError(\"Faltan columnas necesarias (card1, uid_td_D1, TransactionID)\")\n",
    "    df['cumc'] = df.groupby(['card1', 'uid_td_D1']).cumcount()\n",
    "    first_df = df[df['cumc'] == 0].copy()\n",
    "    if len(first_df) > 0:\n",
    "        first_df['uid'] = first_df['TransactionID']\n",
    "    first_df.drop(columns=['cumc'], inplace=True, errors='ignore')\n",
    "    return first_df\n",
    "\n",
    "def append_item_to_uid(nan_df, full_df, v_cols=None):\n",
    "    \"\"\"\n",
    "    Versión secuencial y simplificada de append_item_to_uid del script:\n",
    "    - Para cada item en nan_df intenta encontrar en full_df un grupo (mismo card1, uid_td_D1,\n",
    "      TransactionID < item.TransactionID, DT_day <= item['uid_td_D3']+1)\n",
    "    - Aplica checks simples de addr1/addr2 y (si v_cols provisto) alguna coincidencia en v_cols_fix\n",
    "    - Si encuentra un único uid en df_masked lo asigna.\n",
    "    Devuelve DataFrame con (TransactionID, uid) para los items asignados.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # seguridad si v_cols None\n",
    "    if v_cols is None:\n",
    "        v_cols = []\n",
    "\n",
    "    required = {'card1','uid_td_D1','TransactionID','uid_td_D3','DT_day'}\n",
    "    missing = required - set(nan_df.columns) - set(full_df.columns)\n",
    "    # No lanzar error; solo aviso\n",
    "    # iterate\n",
    "    for _, item in nan_df.iterrows():\n",
    "        try:\n",
    "            mask = (\n",
    "                (full_df['card1'] == item['card1']) &\n",
    "                (full_df['uid_td_D1'] == item['uid_td_D1']) &\n",
    "                (full_df['TransactionID'] < item['TransactionID']) &\n",
    "                (full_df['DT_day'] <= (item['uid_td_D3'] + 1))\n",
    "            )\n",
    "        except Exception:\n",
    "            # si alguna columna faltó o es NaN, saltar item\n",
    "            continue\n",
    "\n",
    "        df_masked = full_df[mask].copy()\n",
    "        if df_masked.empty:\n",
    "            continue\n",
    "\n",
    "        # addr checks: si item tiene addr2/addr1 no-null filtramos por igualdad o NaN en grupo\n",
    "        for col in ['addr2','addr1']:\n",
    "            if col in item.index and pd.notna(item[col]):\n",
    "                df_masked = df_masked[(df_masked[col].isna()) | (df_masked[col] == item[col])]\n",
    "                if df_masked.empty:\n",
    "                    break\n",
    "\n",
    "        if df_masked.empty:\n",
    "            continue\n",
    "\n",
    "        # Si v_cols provistas, chequeamos que al menos una de esas columnas coincida (flexible)\n",
    "        v_match = False\n",
    "        for col in v_cols:\n",
    "            # usamos col+'_fix_ground' si existe, sino la columna original\n",
    "            ground = col + '_fix_ground'\n",
    "            if ground in item.index and ground in df_masked.columns and pd.notna(item[ground]):\n",
    "                if (df_masked[ground] == item[ground]).any():\n",
    "                    v_match = True\n",
    "                    break\n",
    "            elif col in item.index and col in df_masked.columns and pd.notna(item[col]):\n",
    "                if (df_masked[col] == item[col]).any():\n",
    "                    v_match = True\n",
    "                    break\n",
    "\n",
    "        # aceptamos si no hay v_cols (v_match irrelevant) o si hubo algun match\n",
    "        if (len(v_cols) == 0) or v_match:\n",
    "            unique_uids = df_masked['uid'].dropna().unique()\n",
    "            if len(unique_uids) == 1:\n",
    "                results.append((item['TransactionID'], unique_uids[0]))\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return pd.DataFrame(columns=['TransactionID','uid'])\n",
    "    return pd.DataFrame(results, columns=['TransactionID','uid'])\n",
    "\n",
    "def find_multigroup(nan_df, full_df):\n",
    "    \"\"\"\n",
    "    Encuentra posibles grupos multiple candidates (lista de uids) para cada item.\n",
    "    Devuelve DataFrame TransactionID + multi_uid (lista).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for _, item in nan_df.iterrows():\n",
    "        mask = (\n",
    "            (full_df['card1'] == item['card1']) &\n",
    "            (full_df['uid_td_D1'] == item['uid_td_D1']) &\n",
    "            (full_df['TransactionID'] < item['TransactionID']) &\n",
    "            ((full_df['DT_day'] == item['uid_td_D3'] + 1) |\n",
    "             (full_df['DT_day'] == item['uid_td_D3'] - 1) |\n",
    "             (full_df['DT_day'] == item['uid_td_D3']))\n",
    "        )\n",
    "        df_masked = full_df[mask]\n",
    "        if not df_masked.empty:\n",
    "            out.append((item['TransactionID'], list(df_masked['uid'].dropna().unique())))\n",
    "    if len(out) == 0:\n",
    "        return pd.DataFrame(columns=['TransactionID','multi_uid'])\n",
    "    tmp = pd.DataFrame(out, columns=['TransactionID','multi_uid'])\n",
    "    return tmp\n",
    "\n",
    "def find_right_uid(possible_groups, test_item, full_df, v_cols=None):\n",
    "    \"\"\"\n",
    "    Dado un test_item y una lista de uids candidate, escoge el uid \"más compatible\".\n",
    "    Implementa una puntuación simple basada en coincidencias de columnas (versión reducida).\n",
    "    \"\"\"\n",
    "    if v_cols is None:\n",
    "        v_cols = []\n",
    "\n",
    "    features_weight = {\n",
    "        'TransactionAmt': 2,\n",
    "        'card2': 1, 'card3':1, 'card4':1, 'card5':1, 'card6':1,\n",
    "        'uid_td_D2':2, 'uid_td_D10':2, 'uid_td_D11':2, 'uid_td_D15':2,\n",
    "        'C14':1, 'addr1':1, 'addr2':1, 'P_emaildomain':1, 'V313_fix':1\n",
    "    }\n",
    "\n",
    "    scores = {}\n",
    "    for g in possible_groups:\n",
    "        masked = full_df[full_df['uid'] == g]\n",
    "        if masked.empty:\n",
    "            continue\n",
    "        score = 0\n",
    "        for col, w in features_weight.items():\n",
    "            if col in test_item.index and col in masked.columns and pd.notna(test_item[col]):\n",
    "                if test_item[col] in set(masked[col].dropna().values):\n",
    "                    score += w\n",
    "        # V columns check\n",
    "        for col in v_cols:\n",
    "            ground = col + '_fix_ground'\n",
    "            if ground in test_item.index and ground in masked.columns and pd.notna(test_item[ground]):\n",
    "                if test_item[ground] in set(masked[ground].dropna().values):\n",
    "                    score += 1\n",
    "        # almacenamos si el grupo pasa un check minimo\n",
    "        scores[g] = score\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return np.nan\n",
    "    # tomamos el uid con mejor score (si empate, toma el primero)\n",
    "    best_uid = max(scores.items(), key=lambda x: x[1])[0]\n",
    "    return best_uid\n",
    "\n",
    "# -------------------------\n",
    "# 4) Orquestador: genera uid para todo el df\n",
    "# -------------------------\n",
    "def generate_uids_from_df(df, v_cols=None, rounds_assign=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Orquesta todo el proceso: preprocesa, asigna single, first appearances,\n",
    "    luego itera append_item_to_uid varias rondas, luego intenta multigroup resolution.\n",
    "    Devuelve df con columna 'uid' (puede quedar NaN en algunos items).\n",
    "    \"\"\"\n",
    "    # Precondiciones\n",
    "    required_cols = ['TransactionID','card1','TransactionDT','D1']\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Falta columna requerida: {c}\")\n",
    "\n",
    "    # Copias de trabajo\n",
    "    work_df = df.copy().reset_index(drop=True)\n",
    "\n",
    "    # preprocesado (si no corriste preprocess anteriormente)\n",
    "    work_df, detected_v_cols = preprocess_for_uid(work_df)\n",
    "    if v_cols is None:\n",
    "        v_cols = detected_v_cols\n",
    "\n",
    "    # bkp_items = copia para lookups\n",
    "    bkp_items = work_df.copy()\n",
    "    all_items = work_df.copy()\n",
    "\n",
    "    # full_df: filas ya asignadas uid (inicialmente vacio)\n",
    "    full_df = pd.DataFrame(columns=work_df.columns)\n",
    "\n",
    "    # 1) single items\n",
    "    single_items, all_items = assign_single_items(all_items)\n",
    "    if verbose:\n",
    "        print(\"Single transactions:\", len(single_items))\n",
    "\n",
    "    # 2) first appearances (one-first per (card1, uid_td_D1))\n",
    "    first_df = assign_first_appearances(all_items)\n",
    "    if verbose:\n",
    "        print(\"First appearances assigned:\", len(first_df))\n",
    "    full_df = pd.concat([full_df, first_df]).sort_values(by='TransactionID').reset_index(drop=True)\n",
    "\n",
    "    # iteraciones de asignacion por \"match 1-to-1\"\n",
    "    for r in range(rounds_assign):\n",
    "        nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n",
    "        if verbose:\n",
    "            print(f\"Ronda {r} - items por intentar asignar:\", len(nan_df))\n",
    "        if len(nan_df) == 0:\n",
    "            break\n",
    "        assigned = append_item_to_uid(nan_df, full_df, v_cols=v_cols)\n",
    "        if assigned.empty:\n",
    "            if verbose:\n",
    "                print(\"No hay asignaciones en esta ronda.\")\n",
    "            break\n",
    "        # map y append\n",
    "        assigned_map = dict(zip(assigned['TransactionID'], assigned['uid']))\n",
    "        nan_df.loc[nan_df['TransactionID'].isin(assigned['TransactionID']), 'uid'] = nan_df.loc[nan_df['TransactionID'].isin(assigned['TransactionID']), 'TransactionID'].map(assigned_map)\n",
    "        # en el script original asignaban el uid encontrado (no uid=TransactionID), así que mapeamos correctamente:\n",
    "        # Necesitamos extraer uid desde assigned\n",
    "        nan_df.loc[nan_df['TransactionID'].isin(assigned['TransactionID']), 'uid'] = nan_df.loc[nan_df['TransactionID'].isin(assigned['TransactionID']), 'TransactionID'].map(assigned_map)\n",
    "        # corregimos: assigned_map ya tiene uid en assigned['uid']\n",
    "        # append to full_df\n",
    "        to_append = all_items[all_items['TransactionID'].isin(assigned['TransactionID'])].copy()\n",
    "        # sustituimos uid por la encontrada\n",
    "        for tid, uid in zip(assigned['TransactionID'], assigned['uid']):\n",
    "            to_append.loc[to_append['TransactionID']==tid, 'uid'] = uid\n",
    "        full_df = pd.concat([full_df, to_append]).sort_values(by='TransactionID').reset_index(drop=True)\n",
    "        if verbose:\n",
    "            print(\"Assigned items this round:\", len(to_append))\n",
    "\n",
    "    # Multigroup resolution (items that aún no están en full_df)\n",
    "    remaining = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n",
    "    if verbose:\n",
    "        print(\"Items remaining antes de multigroup:\", len(remaining))\n",
    "    if len(remaining) > 0:\n",
    "        # encontrar listas de candidate groups\n",
    "        mg = find_multigroup(remaining, full_df)\n",
    "        if not mg.empty:\n",
    "            # resolver group por group (secuencial)\n",
    "            resolved = []\n",
    "            for _, row in mg.iterrows():\n",
    "                test_id = row['TransactionID']\n",
    "                possible = row['multi_uid']\n",
    "                test_item = all_items[all_items['TransactionID']==test_id].iloc[0]\n",
    "                chosen = find_right_uid(possible, test_item, full_df, v_cols=v_cols)\n",
    "                if pd.notna(chosen):\n",
    "                    resolved.append((test_id, chosen))\n",
    "            if len(resolved) > 0:\n",
    "                tmp = pd.DataFrame(resolved, columns=['TransactionID','uid'])\n",
    "                # anadir los resueltos a full_df\n",
    "                to_append = all_items[all_items['TransactionID'].isin(tmp['TransactionID'])].copy()\n",
    "                for tid, uid in zip(tmp['TransactionID'], tmp['uid']):\n",
    "                    to_append.loc[to_append['TransactionID']==tid, 'uid'] = uid\n",
    "                full_df = pd.concat([full_df, to_append]).sort_values(by='TransactionID').reset_index(drop=True)\n",
    "                if verbose:\n",
    "                    print(\"Multigroup resolved:\", len(to_append))\n",
    "\n",
    "    # Combine final: full_df (asignados), single_items, y los no asignados (opcionales)\n",
    "    final = pd.concat([full_df, single_items]).sort_values(by='TransactionID').reset_index(drop=True)\n",
    "    # opcional: incluir todos los items sin uid (marcarlos NaN) si quieres el DF completo:\n",
    "    merged = df.merge(final[['TransactionID','uid']], on='TransactionID', how='left', suffixes=('','_uid_assigned'))\n",
    "    # si ya existía uid en df la preferimos; sino usamos uid_assigned:\n",
    "    merged['uid'] = merged['uid'].fillna(merged.get('uid_assigned'))\n",
    "    merged.drop(columns=['uid_assigned'], inplace=True, errors='ignore')\n",
    "\n",
    "    if verbose:\n",
    "        assigned_total = merged['uid'].notna().sum()\n",
    "        print(\"UIDs asignados (total):\", assigned_total, \"/\", len(merged))\n",
    "    return merged\n",
    "\n",
    "# -------------------------\n",
    "# 5) Export helper\n",
    "# -------------------------\n",
    "def export_uids_df(df_with_uid, output_path='uids_full_from_csv.csv'):\n",
    "    \"\"\"\n",
    "    Exporta TransactionID + uid\n",
    "    \"\"\"\n",
    "    out = df_with_uid[['TransactionID','uid']].copy()\n",
    "    out.to_csv(output_path, index=False)\n",
    "    print(\"Exportado:\", output_path)\n",
    "    return output_path\n",
    "\n",
    "# -------------------------\n",
    "# 6) FUNCION PRINCIPAL - uso directo\n",
    "# -------------------------\n",
    "def run_pipeline(csv_in='credit_dataset_2_transaccion.csv',\n",
    "                 csv_out='uids_full_from_csv.csv',\n",
    "                 local_test=False,\n",
    "                 rounds_assign=5,\n",
    "                 verbose=True):\n",
    "    df = load_csv(csv_in, local_test=local_test)\n",
    "    df_pre, v_cols = preprocess_for_uid(df)\n",
    "    result = generate_uids_from_df(df_pre, v_cols=v_cols, rounds_assign=rounds_assign, verbose=verbose)\n",
    "    export_path = export_uids_df(result, output_path=csv_out)\n",
    "    return result, export_path\n",
    "\n",
    "# =====================================================================\n",
    "# USO:\n",
    "# - Pegar este bloque en una celda de Jupyter (mismo folder que tu CSV)\n",
    "# - Ejecutar:\n",
    "#     result_df, path = run_pipeline(local_test=False, rounds_assign=5, csv_in='credit_dataset_2_transaccion.csv')\n",
    "# - El CSV producido se llamará por defecto 'uids_full_from_csv.csv'\n",
    "# =====================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996f30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv_light(path='credit_dataset_2_transaccion.csv'):\n",
    "    print(\"Leyendo:\", path)\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def generate_uid_as_tid(df):\n",
    "    \"\"\"\n",
    "    Genera un UID usando card1, addr1, D1n pero en vez de concatenar,\n",
    "    asigna como uid el TransactionID de la primera transacción del grupo.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Crear D1n\n",
    "    df['D1n'] = np.floor(df['TransactionDT'] / (24*60*60)) - df['D1']\n",
    "    \n",
    "    # Crear clave de agrupación\n",
    "    group_key = (\n",
    "        df['card1'].astype(str) + '_' +\n",
    "        df['addr1'].astype(str) + '_' +\n",
    "        df['D1n'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Para cada grupo, tomar el TransactionID mínimo como uid\n",
    "    uid_map = df.groupby(group_key)['TransactionID'].transform('min')\n",
    "    df['uid'] = uid_map\n",
    "    \n",
    "    return df\n",
    "\n",
    "def export_uid(df, path='uids_light_tid.csv'):\n",
    "    df[['TransactionID','uid']].to_csv(path, index=False)\n",
    "    print(\"Exportado:\", path)\n",
    "    return path\n",
    "\n",
    "def run_pipeline_light_tid(csv_in='credit_dataset_2_transaccion.csv',\n",
    "                           csv_out='uids_light_tid.csv'):\n",
    "    df = load_csv_light(csv_in)\n",
    "    df_uid = generate_uid_as_tid(df)\n",
    "    export_path = export_uid(df_uid, csv_out)\n",
    "    return df_uid, export_path\n",
    "\n",
    "# Uso:\n",
    "# result_df, path = run_pipeline_light_tid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "febaaefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: credit_dataset_2_transaccion.csv\n",
      "Exportado: uids_light_tid.csv\n"
     ]
    }
   ],
   "source": [
    "result_df, path = run_pipeline_light_tid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
